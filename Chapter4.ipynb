{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb87f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Architecture Search (NAS) - Code Examples\n",
    "## Chapter 4 Companion Notebook\n",
    "\n",
    "## This notebook contains all the code examples from Chapter 4 of the AutoML book, organized for easy experimentation and learning.\n",
    "\n",
    "### Table of Contents\n",
    "#1. [Setup and Dependencies](#setup)\n",
    "#2. [Search Space Design](#search-spaces)\n",
    "#3. [Search Strategies](#search-strategies)\n",
    "#4. [Performance Estimation](#performance-estimation)\n",
    "#5. [Efficient NAS Techniques](#efficient-nas)\n",
    "#6. [Practical Tools and Applications](#practical-tools)\n",
    "\n",
    "\n",
    "\n",
    "## 1. Setup and Dependencies {#setup}\n",
    "\n",
    "\n",
    "# Core dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# AutoML frameworks (install as needed)\n",
    "# pip install autokeras\n",
    "# pip install optuna\n",
    "# pip install ray[tune]\n",
    "# pip install nni\n",
    "\n",
    "# Optional imports (uncomment as needed)\n",
    "# import autokeras as ak\n",
    "# import tensorflow as tf\n",
    "# import optuna\n",
    "# from ray import tune\n",
    "# from ray.tune.suggest.optuna import OptunaSearch\n",
    "# from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "\n",
    "## 2. Search Space Design {#search-spaces}\n",
    "\n",
    "### 2.1 Cell-Based Search Space with AutoKeras\n",
    "\n",
    "# Note: Requires autokeras installation\n",
    "# pip install autokeras\n",
    "\n",
    "import autokeras as ak\n",
    "\n",
    "def create_nas_model():\n",
    "    \"\"\"\n",
    "    Define a cell-based search space for image classification.\n",
    "    This example shows how to create flexible, scalable search spaces.\n",
    "    \"\"\"\n",
    "    input_node = ak.ImageInput()\n",
    "    \n",
    "    # Search over different cell types\n",
    "    cell_type = ak.Choice(['conv_cell', 'dense_cell', 'mobile_cell'])\n",
    "    \n",
    "    # Each cell can have different configurations\n",
    "    conv_cell = ak.ConvBlock(\n",
    "        num_blocks=ak.Choice([2, 3, 4]),\n",
    "        num_layers=ak.Choice([1, 2]),\n",
    "        filters=ak.Choice([32, 64, 128, 256]),\n",
    "        kernel_size=ak.Choice([3, 5]),\n",
    "        activation=ak.Choice(['relu', 'swish'])\n",
    "    )\n",
    "    \n",
    "    # Stack multiple cells with variable depth\n",
    "    output = conv_cell(input_node)\n",
    "    for i in range(ak.Choice([3, 5, 7])):  # Variable depth\n",
    "        output = conv_cell(output)\n",
    "    \n",
    "    output = ak.ClassificationHead()(output)\n",
    "    return ak.AutoModel(inputs=input_node, outputs=output)\n",
    "\n",
    "# Example usage:\n",
    "# model = create_nas_model()\n",
    "# print(\"Search space created successfully!\")\n",
    "\n",
    "\n",
    "### 2.2 Custom Search Space Definition\n",
    "\n",
    "class SearchSpaceConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for defining custom search spaces.\n",
    "    Demonstrates how to encode architectural choices programmatically.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.layer_types = ['conv', 'depthwise_conv', 'dilated_conv', 'skip_connect']\n",
    "        self.filter_sizes = [16, 32, 64, 128, 256]\n",
    "        self.kernel_sizes = [3, 5, 7]\n",
    "        self.activation_functions = ['relu', 'swish', 'gelu']\n",
    "        self.depth_range = (5, 20)\n",
    "        self.width_multipliers = [0.5, 0.75, 1.0, 1.25]\n",
    "    \n",
    "    def sample_architecture(self, depth=None):\n",
    "        \"\"\"Sample a random architecture from the search space.\"\"\"\n",
    "        if depth is None:\n",
    "            depth = random.randint(*self.depth_range)\n",
    "        \n",
    "        architecture = []\n",
    "        for i in range(depth):\n",
    "            layer_config = {\n",
    "                'type': random.choice(self.layer_types),\n",
    "                'filters': random.choice(self.filter_sizes),\n",
    "                'kernel_size': random.choice(self.kernel_sizes),\n",
    "                'activation': random.choice(self.activation_functions)\n",
    "            }\n",
    "            architecture.append(layer_config)\n",
    "        \n",
    "        return architecture\n",
    "    \n",
    "    def get_search_space_size(self):\n",
    "        \"\"\"Calculate the theoretical size of the search space.\"\"\"\n",
    "        choices_per_layer = (len(self.layer_types) * \n",
    "                           len(self.filter_sizes) * \n",
    "                           len(self.kernel_sizes) * \n",
    "                           len(self.activation_functions))\n",
    "        \n",
    "        total_size = 0\n",
    "        for depth in range(*self.depth_range):\n",
    "            total_size += choices_per_layer ** depth\n",
    "        \n",
    "        return total_size\n",
    "\n",
    "# Example usage:\n",
    "search_config = SearchSpaceConfig()\n",
    "sample_arch = search_config.sample_architecture(depth=10)\n",
    "print(f\"Sample architecture: {sample_arch[:3]}...\")  # Show first 3 layers\n",
    "print(f\"Search space size: {search_config.get_search_space_size():.2e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. Search Strategies {#search-strategies}\n",
    "\n",
    "### 3.1 Evolutionary Neural Architecture Search\n",
    "\n",
    "def evolutionary_nas(population_size=50, generations=100, search_config=None):\n",
    "    \"\"\"\n",
    "    Simplified evolutionary NAS implementation.\n",
    "    \n",
    "    Args:\n",
    "        population_size: Number of architectures in each generation\n",
    "        generations: Number of evolutionary cycles\n",
    "        search_config: SearchSpaceConfig instance\n",
    "    \n",
    "    Returns:\n",
    "        Best architecture found\n",
    "    \"\"\"\n",
    "    if search_config is None:\n",
    "        search_config = SearchSpaceConfig()\n",
    "    \n",
    "    # Initialize random population\n",
    "    population = []\n",
    "    for _ in range(population_size):\n",
    "        arch = search_config.sample_architecture()\n",
    "        population.append(arch)\n",
    "    \n",
    "    fitness_history = []\n",
    "    \n",
    "    for generation in range(generations):\n",
    "        # Evaluate all architectures (placeholder - would train real models)\n",
    "        fitness_scores = [evaluate_architecture_placeholder(arch) for arch in population]\n",
    "        fitness_history.append(max(fitness_scores))\n",
    "        \n",
    "        # Select top performers (top 50%)\n",
    "        combined = list(zip(population, fitness_scores))\n",
    "        combined.sort(key=lambda x: x[1], reverse=True)\n",
    "        survivors = [arch for arch, _ in combined[:population_size//2]]\n",
    "        \n",
    "        # Generate next generation through mutation\n",
    "        new_population = []\n",
    "        for parent in survivors:\n",
    "            # Keep parent\n",
    "            new_population.append(parent)\n",
    "            # Create mutated offspring\n",
    "            child = mutate_architecture(parent, search_config)\n",
    "            new_population.append(child)\n",
    "        \n",
    "        population = new_population\n",
    "        \n",
    "        if generation % 10 == 0:\n",
    "            print(f\"Generation {generation}: Best fitness = {max(fitness_scores):.4f}\")\n",
    "    \n",
    "    # Return best architecture\n",
    "    final_scores = [evaluate_architecture_placeholder(arch) for arch in population]\n",
    "    best_idx = np.argmax(final_scores)\n",
    "    \n",
    "    return population[best_idx], fitness_history\n",
    "\n",
    "def mutate_architecture(architecture, search_config, mutation_rate=0.3):\n",
    "    \"\"\"\n",
    "    Mutate an architecture by randomly changing some layers.\n",
    "    \n",
    "    Args:\n",
    "        architecture: List of layer configurations\n",
    "        search_config: SearchSpaceConfig instance\n",
    "        mutation_rate: Probability of mutating each layer\n",
    "    \n",
    "    Returns:\n",
    "        Mutated architecture\n",
    "    \"\"\"\n",
    "    mutated = []\n",
    "    for layer in architecture:\n",
    "        if random.random() < mutation_rate:\n",
    "            # Create new random layer\n",
    "            new_layer = {\n",
    "                'type': random.choice(search_config.layer_types),\n",
    "                'filters': random.choice(search_config.filter_sizes),\n",
    "                'kernel_size': random.choice(search_config.kernel_sizes),\n",
    "                'activation': random.choice(search_config.activation_functions)\n",
    "            }\n",
    "            mutated.append(new_layer)\n",
    "        else:\n",
    "            # Keep original layer\n",
    "            mutated.append(layer.copy())\n",
    "    \n",
    "    return mutated\n",
    "\n",
    "def evaluate_architecture_placeholder(architecture):\n",
    "    \"\"\"\n",
    "    Placeholder evaluation function.\n",
    "    In practice, this would train the architecture and return validation accuracy.\n",
    "    \"\"\"\n",
    "    # Simulate architecture evaluation with some heuristics\n",
    "    score = 0.5  # Base score\n",
    "    \n",
    "    # Reward skip connections\n",
    "    skip_connections = sum(1 for layer in architecture if layer['type'] == 'skip_connect')\n",
    "    score += skip_connections * 0.02\n",
    "    \n",
    "    # Penalize very deep or very shallow networks\n",
    "    depth = len(architecture)\n",
    "    if 8 <= depth <= 15:\n",
    "        score += 0.1\n",
    "    \n",
    "    # Add some randomness to simulate training variance\n",
    "    score += random.gauss(0, 0.05)\n",
    "    \n",
    "    return max(0, min(1, score))\n",
    "\n",
    "# Example usage:\n",
    "# best_arch, history = evolutionary_nas(population_size=20, generations=50)\n",
    "# print(f\"Best architecture found: {len(best_arch)} layers\")\n",
    "\n",
    "\n",
    "### 3.2 Differentiable Architecture Search (DARTS)\n",
    "\n",
    "class MixedOperation(nn.Module):\n",
    "    \"\"\"\n",
    "    A mixed operation that combines multiple candidate operations.\n",
    "    Core component of differentiable NAS.\n",
    "    \"\"\"\n",
    "    def __init__(self, operations, channels):\n",
    "        super().__init__()\n",
    "        self.operations = nn.ModuleList()\n",
    "        \n",
    "        # Create all candidate operations\n",
    "        for op_name in operations:\n",
    "            if op_name == 'conv3':\n",
    "                op = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "            elif op_name == 'conv5':\n",
    "                op = nn.Conv2d(channels, channels, 5, padding=2)\n",
    "            elif op_name == 'maxpool':\n",
    "                op = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "            elif op_name == 'skip':\n",
    "                op = nn.Identity()\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown operation: {op_name}\")\n",
    "            \n",
    "            self.operations.append(op)\n",
    "        \n",
    "        # Learnable weights for combining operations\n",
    "        self.weights = nn.Parameter(torch.randn(len(operations)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Weighted combination of all operations\n",
    "        weights_softmax = F.softmax(self.weights, dim=0)\n",
    "        output = sum(w * op(x) for w, op in zip(weights_softmax, self.operations))\n",
    "        return output\n",
    "\n",
    "class DARTSCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A DARTS cell that searches over different operation combinations.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels=64, num_nodes=4):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.operations = ['conv3', 'conv5', 'maxpool', 'skip']\n",
    "        \n",
    "        # Create mixed operations for each edge in the cell\n",
    "        self.mixed_ops = nn.ModuleList()\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 2):  # Each node connects to all previous nodes + 2 inputs\n",
    "                mixed_op = MixedOperation(self.operations, channels)\n",
    "                self.mixed_ops.append(mixed_op)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Process through the cell DAG\n",
    "        states = [x, x]  # Two initial states\n",
    "        \n",
    "        op_idx = 0\n",
    "        for i in range(self.num_nodes):\n",
    "            # Collect inputs from all previous nodes\n",
    "            node_inputs = []\n",
    "            for j in range(i + 2):\n",
    "                node_input = self.mixed_ops[op_idx](states[j])\n",
    "                node_inputs.append(node_input)\n",
    "                op_idx += 1\n",
    "            \n",
    "            # Combine inputs for this node\n",
    "            node_output = sum(node_inputs)\n",
    "            states.append(node_output)\n",
    "        \n",
    "        # Return concatenation of final states\n",
    "        return torch.cat(states[-self.num_nodes:], dim=1)\n",
    "    \n",
    "    def get_best_architecture(self):\n",
    "        \"\"\"Extract the best discrete architecture from learned weights.\"\"\"\n",
    "        best_ops = []\n",
    "        op_idx = 0\n",
    "        \n",
    "        for i in range(self.num_nodes):\n",
    "            node_ops = []\n",
    "            for j in range(i + 2):\n",
    "                weights = self.mixed_ops[op_idx].weights\n",
    "                best_op_idx = torch.argmax(weights)\n",
    "                best_op = self.operations[best_op_idx]\n",
    "                node_ops.append((j, best_op))\n",
    "                op_idx += 1\n",
    "            best_ops.append(node_ops)\n",
    "        \n",
    "        return best_ops\n",
    "\n",
    "class DARTSNetwork(nn.Module):\n",
    "    \"\"\"Complete network using DARTS cells.\"\"\"\n",
    "    def __init__(self, num_classes=10, channels=64, num_cells=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stem\n",
    "        self.stem = nn.Conv2d(3, channels, 3, padding=1)\n",
    "        \n",
    "        # Stack of DARTS cells\n",
    "        self.cells = nn.ModuleList()\n",
    "        for _ in range(num_cells):\n",
    "            cell = DARTSCell(channels)\n",
    "            self.cells.append(cell)\n",
    "        \n",
    "        # Classifier head\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(channels * 4, num_classes)  # 4 from concatenation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        \n",
    "        for cell in self.cells:\n",
    "            x = cell(x)\n",
    "        \n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "# model = DARTSNetwork(num_classes=10)\n",
    "# print(\"DARTS model created with\", sum(p.numel() for p in model.parameters()), \"parameters\")\n",
    "\n",
    "\n",
    "## 4. Performance Estimation {#performance-estimation}\n",
    "\n",
    "### 4.1 Successive Halving for Multi-fidelity Evaluation\n",
    "\n",
    "def successive_halving_nas(architectures, max_epochs=100, reduction_factor=2):\n",
    "    \"\"\"\n",
    "    Implement Successive Halving for efficient architecture evaluation.\n",
    "    \n",
    "    Args:\n",
    "        architectures: List of architectures to evaluate\n",
    "        max_epochs: Maximum training epochs for final candidates\n",
    "        reduction_factor: Factor by which to reduce population each round\n",
    "    \n",
    "    Returns:\n",
    "        Best performing architecture\n",
    "    \"\"\"\n",
    "    candidates = architectures.copy()\n",
    "    epochs = max_epochs // (reduction_factor ** 3)  # Start with fewer epochs\n",
    "    \n",
    "    print(f\"Starting Successive Halving with {len(candidates)} candidates\")\n",
    "    \n",
    "    round_num = 1\n",
    "    while len(candidates) > 1 and epochs <= max_epochs:\n",
    "        print(f\"\\nRound {round_num}: Evaluating {len(candidates)} candidates for {epochs} epochs\")\n",
    "        \n",
    "        # Train all candidates for current epoch budget\n",
    "        results = []\n",
    "        for i, arch in enumerate(candidates):\n",
    "            score = train_and_evaluate_placeholder(arch, epochs=epochs)\n",
    "            results.append((arch, score))\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Evaluated {i+1}/{len(candidates)} candidates\")\n",
    "        \n",
    "        # Keep top performers\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        keep_count = max(1, len(results) // reduction_factor)\n",
    "        candidates = [arch for arch, _ in results[:keep_count]]\n",
    "        \n",
    "        print(f\"  Best score this round: {results[0][1]:.4f}\")\n",
    "        print(f\"  Keeping top {keep_count} candidates\")\n",
    "        \n",
    "        # Double the training budget for next round\n",
    "        epochs *= reduction_factor\n",
    "        round_num += 1\n",
    "    \n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "def train_and_evaluate_placeholder(architecture, epochs):\n",
    "    \"\"\"\n",
    "    Placeholder for training and evaluating an architecture.\n",
    "    In practice, this would involve actual model training.\n",
    "    \"\"\"\n",
    "    # Simulate training time based on epochs\n",
    "    import time\n",
    "    time.sleep(0.01 * epochs)  # Simulate training\n",
    "    \n",
    "    # Simulate performance based on architecture and training time\n",
    "    base_score = evaluate_architecture_placeholder(architecture)\n",
    "    \n",
    "    # Improve score based on training epochs (with diminishing returns)\n",
    "    epoch_bonus = 0.1 * (1 - np.exp(-epochs / 50))\n",
    "    final_score = min(1.0, base_score + epoch_bonus)\n",
    "    \n",
    "    # Add some noise to simulate training variance\n",
    "    noise = random.gauss(0, 0.02)\n",
    "    return max(0, final_score + noise)\n",
    "\n",
    "# Example usage:\n",
    "# sample_architectures = [search_config.sample_architecture() for _ in range(27)]\n",
    "# best_architecture = successive_halving_nas(sample_architectures, max_epochs=81)\n",
    "# print(f\"Best architecture has {len(best_architecture)} layers\")\n",
    "\n",
    "\n",
    "### 4.2 One-Shot Architecture Search (Weight Sharing)\n",
    "\n",
    "class Supernet(nn.Module):\n",
    "    \"\"\"\n",
    "    Supernet for one-shot architecture search.\n",
    "    Contains all possible operations and shares weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, search_space_config, input_channels=3, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.search_config = search_space_config\n",
    "        \n",
    "        # Create operations for all possible layer types\n",
    "        self.operations = nn.ModuleDict()\n",
    "        \n",
    "        # Define operation implementations\n",
    "        for layer_type in search_space_config.layer_types:\n",
    "            ops_for_type = nn.ModuleDict()\n",
    "            \n",
    "            for filters in search_space_config.filter_sizes:\n",
    "                for kernel_size in search_space_config.kernel_sizes:\n",
    "                    op_name = f\"{layer_type}_{filters}_{kernel_size}\"\n",
    "                    \n",
    "                    if layer_type == 'conv':\n",
    "                        op = nn.Conv2d(filters, filters, kernel_size, padding=kernel_size//2)\n",
    "                    elif layer_type == 'depthwise_conv':\n",
    "                        op = nn.Conv2d(filters, filters, kernel_size, groups=filters, padding=kernel_size//2)\n",
    "                    elif layer_type == 'dilated_conv':\n",
    "                        op = nn.Conv2d(filters, filters, kernel_size, dilation=2, padding=kernel_size)\n",
    "                    elif layer_type == 'skip_connect':\n",
    "                        op = nn.Identity()\n",
    "                    \n",
    "                    ops_for_type[op_name] = op\n",
    "            \n",
    "            self.operations[layer_type] = ops_for_type\n",
    "        \n",
    "        # Stem and head\n",
    "        self.stem = nn.Conv2d(input_channels, 64, 3, padding=1)\n",
    "        self.head = nn.Linear(64, num_classes)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "    \n",
    "    def forward(self, x, architecture):\n",
    "        \"\"\"\n",
    "        Forward pass with specific architecture.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            architecture: List of layer configurations\n",
    "        \"\"\"\n",
    "        x = self.stem(x)\n",
    "        \n",
    "        for layer_config in architecture:\n",
    "            layer_type = layer_config['type']\n",
    "            filters = layer_config['filters']\n",
    "            kernel_size = layer_config['kernel_size']\n",
    "            \n",
    "            op_name = f\"{layer_type}_{filters}_{kernel_size}\"\n",
    "            \n",
    "            if op_name in self.operations[layer_type]:\n",
    "                # Adjust channels if needed\n",
    "                if x.size(1) != filters:\n",
    "                    x = F.adaptive_avg_pool2d(x, x.size()[2:])\n",
    "                    if x.size(1) < filters:\n",
    "                        # Pad channels\n",
    "                        padding = filters - x.size(1)\n",
    "                        x = F.pad(x, (0, 0, 0, 0, 0, padding))\n",
    "                    elif x.size(1) > filters:\n",
    "                        # Reduce channels\n",
    "                        x = x[:, :filters, :, :]\n",
    "                \n",
    "                x = self.operations[layer_type][op_name](x)\n",
    "                x = F.relu(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def evaluate_architecture_fast(supernet, architecture, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate architecture using pre-trained supernet weights.\n",
    "    This is much faster than training from scratch.\n",
    "    \"\"\"\n",
    "    supernet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            outputs = supernet(data, architecture)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# Example usage (requires actual data):\n",
    "# supernet = Supernet(search_config)\n",
    "# sample_arch = search_config.sample_architecture(depth=5)\n",
    "# print(\"Supernet created with\", sum(p.numel() for p in supernet.parameters()), \"parameters\")\n",
    "\n",
    "\n",
    "### 4.3 Learning Curve Extrapolation\n",
    "\n",
    "def exponential_curve(x, a, b, c):\n",
    "    \"\"\"Exponential saturation curve for learning curve fitting.\"\"\"\n",
    "    return a * (1 - np.exp(-b * x)) + c\n",
    "\n",
    "def predict_final_accuracy(early_accuracies, target_epochs, plot=False):\n",
    "    \"\"\"\n",
    "    Predict final accuracy from early training epochs.\n",
    "    \n",
    "    Args:\n",
    "        early_accuracies: List of validation accuracies from early epochs\n",
    "        target_epochs: Number of epochs to predict performance for\n",
    "        plot: Whether to visualize the fit\n",
    "    \n",
    "    Returns:\n",
    "        Predicted final accuracy\n",
    "    \"\"\"\n",
    "    if len(early_accuracies) < 3:\n",
    "        return early_accuracies[-1]  # Need at least 3 points to fit\n",
    "    \n",
    "    epochs = np.arange(1, len(early_accuracies) + 1)\n",
    "    \n",
    "    # Fit exponential curve to early training\n",
    "    try:\n",
    "        # Initial parameter guess\n",
    "        initial_guess = [max(early_accuracies), 0.1, min(early_accuracies)]\n",
    "        params, _ = curve_fit(exponential_curve, epochs, early_accuracies, \n",
    "                            p0=initial_guess, maxfev=1000)\n",
    "        \n",
    "        predicted = exponential_curve(target_epochs, *params)\n",
    "        predicted = max(0, min(1, predicted))  # Clamp to valid range\n",
    "        \n",
    "        if plot:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(epochs, early_accuracies, 'bo', label='Observed')\n",
    "            \n",
    "            extended_epochs = np.linspace(1, target_epochs, 100)\n",
    "            predicted_curve = exponential_curve(extended_epochs, *params)\n",
    "            plt.plot(extended_epochs, predicted_curve, 'r-', label='Fitted curve')\n",
    "            \n",
    "            plt.axvline(x=len(early_accuracies), color='g', linestyle='--', \n",
    "                       label='Prediction point')\n",
    "            plt.axhline(y=predicted, color='r', linestyle='--', \n",
    "                       label=f'Predicted: {predicted:.3f}')\n",
    "            \n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Learning Curve Extrapolation')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        \n",
    "        return predicted\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Curve fitting failed: {e}\")\n",
    "        return early_accuracies[-1]  # Fallback to last observed value\n",
    "\n",
    "def simulate_learning_curve(true_final_accuracy=0.85, noise_level=0.02):\n",
    "    \"\"\"Simulate a realistic learning curve for testing.\"\"\"\n",
    "    epochs = np.arange(1, 21)  # 20 epochs\n",
    "    \n",
    "    # Generate realistic learning curve\n",
    "    base_curve = true_final_accuracy * (1 - np.exp(-epochs / 8))\n",
    "    \n",
    "    # Add realistic noise (decreasing over time)\n",
    "    noise = np.random.normal(0, noise_level / np.sqrt(epochs))\n",
    "    noisy_curve = base_curve + noise\n",
    "    \n",
    "    return np.maximum(0, np.minimum(1, noisy_curve))\n",
    "\n",
    "# Example usage:\n",
    "# Simulate early training results\n",
    "simulated_accuracies = simulate_learning_curve()\n",
    "early_epochs = simulated_accuracies[:5]  # First 5 epochs\n",
    "true_final = simulated_accuracies[-1]  # True final accuracy\n",
    "\n",
    "predicted_final = predict_final_accuracy(early_epochs, target_epochs=20, plot=True)\n",
    "print(f\"Predicted final accuracy: {predicted_final:.3f}\")\n",
    "print(f\"True final accuracy: {true_final:.3f}\")\n",
    "print(f\"Prediction error: {abs(predicted_final - true_final):.3f}\")\n",
    "\n",
    "\n",
    "### 4.4 Zero-Cost Proxies\n",
    "\n",
    "def snip_score(model, data_sample):\n",
    "    \"\"\"\n",
    "    Compute SNIP score for architecture ranking.\n",
    "    SNIP measures gradient magnitudes after a single forward-backward pass.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Single forward-backward pass\n",
    "    data, targets = data_sample\n",
    "    outputs = model(data)\n",
    "    loss = F.cross_entropy(outputs, targets)\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = torch.autograd.grad(loss, model.parameters(), create_graph=False)\n",
    "    \n",
    "    # SNIP score is sum of gradient magnitudes\n",
    "    score = sum(torch.sum(torch.abs(grad)) for grad in gradients if grad is not None)\n",
    "    return score.item()\n",
    "\n",
    "def gradient_norm_proxy(model, data_sample):\n",
    "    \"\"\"Simple gradient norm proxy for architecture evaluation.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    data, targets = data_sample\n",
    "    outputs = model(data)\n",
    "    loss = F.cross_entropy(outputs, targets)\n",
    "    \n",
    "    # Compute gradients\n",
    "    total_norm = 0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            param_norm = param.grad.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    \n",
    "    return total_norm ** 0.5\n",
    "\n",
    "def connectivity_proxy(architecture):\n",
    "    \"\"\"\n",
    "    Measure architecture connectivity as a proxy for performance.\n",
    "    More connected architectures often perform better.\n",
    "    \"\"\"\n",
    "    skip_connections = sum(1 for layer in architecture if layer['type'] == 'skip_connect')\n",
    "    total_layers = len(architecture)\n",
    "    \n",
    "    connectivity_ratio = skip_connections / max(1, total_layers)\n",
    "    return connectivity_ratio\n",
    "\n",
    "class ZeroCostFilter:\n",
    "    \"\"\"\n",
    "    Multi-proxy filter for rapid architecture screening.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.proxies = {\n",
    "            'connectivity': connectivity_proxy,\n",
    "            # Add more proxies as functions that take (model, data) or just (architecture)\n",
    "        }\n",
    "    \n",
    "    def filter_architectures(self, architectures, keep_fraction=0.1):\n",
    "        \"\"\"\n",
    "        Filter architectures using multiple zero-cost proxies.\n",
    "        \n",
    "        Args:\n",
    "            architectures: List of architectures to filter\n",
    "            keep_fraction: Fraction of architectures to keep\n",
    "        \n",
    "        Returns:\n",
    "            Filtered list of architectures\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # Compute proxy scores\n",
    "        for i, arch in enumerate(architectures):\n",
    "            proxy_scores = []\n",
    "            \n",
    "            # Connectivity proxy (no model needed)\n",
    "            conn_score = connectivity_proxy(arch)\n",
    "            proxy_scores.append(conn_score)\n",
    "            \n",
    "            # For model-based proxies, you would create the model and evaluate:\n",
    "            # model = create_model_from_architecture(arch)\n",
    "            # snip = snip_score(model, data_sample)\n",
    "            # proxy_scores.append(snip)\n",
    "            \n",
    "            # Combine scores (simple average here)\n",
    "            scores[i] = np.mean(proxy_scores)\n",
    "        \n",
    "        # Sort by score and keep top fraction\n",
    "        sorted_indices = sorted(range(len(architectures)), \n",
    "                              key=lambda i: scores[i], reverse=True)\n",
    "        keep_count = int(len(architectures) * keep_fraction)\n",
    "        \n",
    "        filtered_architectures = [architectures[i] for i in sorted_indices[:keep_count]]\n",
    "        \n",
    "        return filtered_architectures\n",
    "\n",
    "# Example usage:\n",
    "filter_system = ZeroCostFilter()\n",
    "sample_architectures = [search_config.sample_architecture() for _ in range(100)]\n",
    "filtered_archs = filter_system.filter_architectures(sample_architectures, keep_fraction=0.2)\n",
    "print(f\"Filtered {len(sample_architectures)} architectures down to {len(filtered_archs)}\")\n",
    "\n",
    "\n",
    "## 5. Efficient NAS Techniques {#efficient-nas}\n",
    "\n",
    "### 5.1 Supernet Training with Balanced Sampling\n",
    "class SupernetTraining:\n",
    "    \"\"\"\n",
    "    Training system for supernets with balanced operation sampling.\n",
    "    Prevents bias toward simple operations during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, supernet, search_space):\n",
    "        self.supernet = supernet\n",
    "        self.search_space = search_space\n",
    "        self.operation_counters = {}  # Track operation usage\n",
    "        \n",
    "    def compute_balanced_weights(self, operations):\n",
    "        \"\"\"\n",
    "        Compute sampling weights to balance operation usage.\n",
    "        \"\"\"\n",
    "        if not self.operation_counters:\n",
    "            # Equal weights initially\n",
    "            return np.ones(len(operations)) / len(operations)\n",
    "        \n",
    "        # Inverse frequency weighting\n",
    "        weights = []\n",
    "        for op in operations:\n",
    "            count = self.operation_counters.get(op, 0)\n",
    "            weight = 1.0 / (count + 1)  # +1 to avoid division by zero\n",
    "            weights.append(weight)\n",
    "        \n",
    "        # Normalize\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def sample_architecture(self):\n",
    "        \"\"\"Sample architecture ensuring balanced operation usage.\"\"\"\n",
    "        architecture = []\n",
    "        \n",
    "        for layer_idx in range(10):  # Fixed depth for example\n",
    "            # Get available operations for this layer\n",
    "            ops = self.search_space.layer_types\n",
    "            \n",
    "            # Balance operation sampling to prevent bias\n",
    "            op_weights = self.compute_balanced_weights(ops)\n",
    "            chosen_op = np.random.choice(ops, p=op_weights)\n",
    "            \n",
    "            # Sample other parameters\n",
    "            layer_config = {\n",
    "                'type': chosen_op,\n",
    "                'filters': random.choice(self.search_space.filter_sizes),\n",
    "                'kernel_size': random.choice(self.search_space.kernel_sizes),\n",
    "                'activation': random.choice(self.search_space.activation_functions)\n",
    "            }\n",
    "            \n",
    "            architecture.append(layer_config)\n",
    "            \n",
    "            # Update counter\n",
    "            self.operation_counters[chosen_op] = self.operation_counters.get(chosen_op, 0) + 1\n",
    "        \n",
    "        return architecture\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Single training step with architecture sampling.\"\"\"\n",
    "        # Sample different architecture for each training step\n",
    "        architecture = self.sample_architecture()\n",
    "        \n",
    "        # Forward pass with sampled architecture\n",
    "        inputs, targets = batch\n",
    "        outputs = self.supernet(inputs, architecture)\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        \n",
    "        # Backward pass updates only active operations\n",
    "        loss.backward()\n",
    "        return loss.item()\n",
    "    \n",
    "    def get_operation_statistics(self):\n",
    "        \"\"\"Get statistics about operation sampling.\"\"\"\n",
    "        total_samples = sum(self.operation_counters.values())\n",
    "        stats = {}\n",
    "        \n",
    "        for op, count in self.operation_counters.items():\n",
    "            stats[op] = {\n",
    "                'count': count,\n",
    "                'frequency': count / total_samples if total_samples > 0 else 0\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Example usage:\n",
    "# supernet = Supernet(search_config)\n",
    "# trainer = SupernetTraining(supernet, search_config)\n",
    "# \n",
    "# # Simulate training\n",
    "# for step in range(100):\n",
    "#     # Create dummy batch\n",
    "#     batch = (torch.randn(4, 3, 32, 32), torch.randint(0, 10, (4,)))\n",
    "#     loss = trainer.train_step(batch)\n",
    "#     \n",
    "#     if step % 20 == 0:\n",
    "#         print(f\"Step {step}, Loss: {loss:.4f}\")\n",
    "# \n",
    "# # Check operation balance\n",
    "# stats = trainer.get_operation_statistics()\n",
    "# for op, stat in stats.items():\n",
    "#     print(f\"{op}: {stat['frequency']:.3f}\")\n",
    "\n",
    "### 5.2 Once-For-All (OFA) Networks\n",
    "\n",
    "class EfficiencyPredictor:\n",
    "    \"\"\"\n",
    "    Predictor for hardware efficiency metrics (latency, memory, energy).\n",
    "    In practice, this would be trained on actual hardware measurements.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.latency_base = 10  # Base latency in ms\n",
    "        self.memory_base = 100  # Base memory in MB\n",
    "        self.energy_base = 50   # Base energy in mJ\n",
    "    \n",
    "    def predict_latency(self, architecture):\n",
    "        \"\"\"Predict inference latency for architecture.\"\"\"\n",
    "        complexity = 0\n",
    "        for layer in architecture:\n",
    "            if layer['type'] == 'conv':\n",
    "                complexity += layer['filters'] * layer['kernel_size']**2\n",
    "            elif layer['type'] == 'depthwise_conv':\n",
    "                complexity += layer['filters'] * layer['kernel_size']**2 * 0.3\n",
    "            elif layer['type'] == 'skip_connect':\n",
    "                complexity += 1\n",
    "        \n",
    "        return self.latency_base + complexity * 0.01\n",
    "    \n",
    "    def predict_memory(self, architecture):\n",
    "        \"\"\"Predict memory usage for architecture.\"\"\"\n",
    "        memory = self.memory_base\n",
    "        for layer in architecture:\n",
    "            memory += layer['filters'] * 0.5  # Simplified calculation\n",
    "        return memory\n",
    "    \n",
    "    def predict_energy(self, architecture):\n",
    "        \"\"\"Predict energy consumption for architecture.\"\"\"\n",
    "        energy = self.energy_base\n",
    "        for layer in architecture:\n",
    "            if layer['type'] in ['conv', 'depthwise_conv']:\n",
    "                energy += layer['filters'] * layer['kernel_size'] * 0.1\n",
    "        return energy\n",
    "\n",
    "class OFANetwork:\n",
    "    \"\"\"\n",
    "    Once-For-All network that can extract specialized sub-networks\n",
    "    for different deployment constraints.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=20, max_width=320):\n",
    "        self.supernet = None  # Would be actual supernet\n",
    "        self.efficiency_predictor = EfficiencyPredictor()\n",
    "        self.max_depth = max_depth\n",
    "        self.max_width = max_width\n",
    "    \n",
    "    def extract_subnet(self, constraints):\n",
    "        \"\"\"\n",
    "        Extract optimal subnet for given constraints.\n",
    "        \n",
    "        Args:\n",
    "            constraints: Dict with keys like 'max_latency', 'max_memory', etc.\n",
    "        \n",
    "        Returns:\n",
    "            Architecture configuration that meets constraints\n",
    "        \"\"\"\n",
    "        best_arch = None\n",
    "        best_score = 0\n",
    "        \n",
    "        # Search over possible configurations\n",
    "        for depth in range(5, min(self.max_depth, constraints.get('max_depth', 20))):\n",
    "            for width_mult in [0.5, 0.75, 1.0]:\n",
    "                # Generate architecture with these specifications\n",
    "                arch = self.generate_architecture(depth, width_mult)\n",
    "                \n",
    "                # Check if architecture meets constraints\n",
    "                if self.meets_constraints(arch, constraints):\n",
    "                    # Estimate accuracy (would use actual prediction in practice)\n",
    "                    score = self.estimate_accuracy(arch)\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_arch = arch\n",
    "                        best_score = score\n",
    "        \n",
    "        return best_arch\n",
    "    \n",
    "    def generate_architecture(self, depth, width_multiplier):\n",
    "        \"\"\"Generate architecture with specified depth and width.\"\"\"\n",
    "        base_filters = int(64 * width_multiplier)\n",
    "        \n",
    "        architecture = []\n",
    "        for i in range(depth):\n",
    "            # Gradually increase filters with depth\n",
    "            filters = min(base_filters * (2 ** (i // 3)), self.max_width)\n",
    "            \n",
    "            layer_config = {\n",
    "                'type': random.choice(['conv', 'depthwise_conv', 'skip_connect']),\n",
    "                'filters': filters,\n",
    "                'kernel_size': random.choice([3, 5]),\n",
    "                'activation': 'relu'\n",
    "            }\n",
    "            architecture.append(layer_config)\n",
    "        \n",
    "        return architecture\n",
    "    \n",
    "    def meets_constraints(self, architecture, constraints):\n",
    "        \"\"\"Check if architecture satisfies deployment constraints.\"\"\"\n",
    "        latency = self.efficiency_predictor.predict_latency(architecture)\n",
    "        memory = self.efficiency_predictor.predict_memory(architecture)\n",
    "        energy = self.efficiency_predictor.predict_energy(architecture)\n",
    "        \n",
    "        checks = []\n",
    "        if 'max_latency' in constraints:\n",
    "            checks.append(latency <= constraints['max_latency'])\n",
    "        if 'max_memory' in constraints:\n",
    "            checks.append(memory <= constraints['max_memory'])\n",
    "        if 'max_energy' in constraints:\n",
    "            checks.append(energy <= constraints['max_energy'])\n",
    "        \n",
    "        return all(checks)\n",
    "    \n",
    "    def estimate_accuracy(self, architecture):\n",
    "        \"\"\"Estimate accuracy for architecture (placeholder).\"\"\"\n",
    "        # Simple heuristic based on architecture properties\n",
    "        base_score = 0.7\n",
    "        \n",
    "        # Reward balanced depth\n",
    "        depth = len(architecture)\n",
    "        if 8 <= depth <= 15:\n",
    "            base_score += 0.1\n",
    "        \n",
    "        # Reward skip connections\n",
    "        skip_ratio = sum(1 for layer in architecture if layer['type'] == 'skip_connect') / depth\n",
    "        base_score += skip_ratio * 0.1\n",
    "        \n",
    "        return min(1.0, base_score + random.gauss(0, 0.02))\n",
    "\n",
    "# Example usage:\n",
    "ofa = OFANetwork()\n",
    "\n",
    "# Define deployment constraints\n",
    "mobile_constraints = {\n",
    "    'max_latency': 50,   # 50ms\n",
    "    'max_memory': 200,   # 200MB\n",
    "    'max_energy': 100    # 100mJ\n",
    "}\n",
    "\n",
    "server_constraints = {\n",
    "    'max_latency': 10,   # 10ms\n",
    "    'max_memory': 1000,  # 1GB\n",
    "    'max_energy': 500    # 500mJ\n",
    "}\n",
    "\n",
    "# Extract specialized architectures\n",
    "mobile_arch = ofa.extract_subnet(mobile_constraints)\n",
    "server_arch = ofa.extract_subnet(server_constraints)\n",
    "\n",
    "print(f\"Mobile architecture: {len(mobile_arch)} layers\")\n",
    "print(f\"Server architecture: {len(server_arch)} layers\")\n",
    "\n",
    "# Compare efficiency\n",
    "mobile_latency = ofa.efficiency_predictor.predict_latency(mobile_arch)\n",
    "server_latency = ofa.efficiency_predictor.predict_latency(server_arch)\n",
    "\n",
    "print(f\"Mobile latency: {mobile_latency:.1f}ms\")\n",
    "print(f\"Server latency: {server_latency:.1f}ms\")\n",
    "\n",
    "### 5.3 Progressive Search\n",
    "\n",
    "class ProgressiveNAS:\n",
    "    \"\"\"\n",
    "    Progressive NAS that starts simple and gradually increases complexity.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.search_stages = [\n",
    "            {\n",
    "                'name': 'Basic',\n",
    "                'max_depth': 5,\n",
    "                'operations': ['conv', 'skip_connect'],\n",
    "                'max_filters': 64,\n",
    "                'trials': 20\n",
    "            },\n",
    "            {\n",
    "                'name': 'Intermediate', \n",
    "                'max_depth': 10,\n",
    "                'operations': ['conv', 'depthwise_conv', 'skip_connect'],\n",
    "                'max_filters': 128,\n",
    "                'trials': 30\n",
    "            },\n",
    "            {\n",
    "                'name': 'Advanced',\n",
    "                'max_depth': 20,\n",
    "                'operations': ['conv', 'depthwise_conv', 'dilated_conv', 'skip_connect'],\n",
    "                'max_filters': 256,\n",
    "                'trials': 50\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def create_search_space(self, stage_config):\n",
    "        \"\"\"Create search space configuration for a specific stage.\"\"\"\n",
    "        config = SearchSpaceConfig()\n",
    "        config.layer_types = stage_config['operations']\n",
    "        config.filter_sizes = [f for f in config.filter_sizes if f <= stage_config['max_filters']]\n",
    "        config.depth_range = (3, stage_config['max_depth'])\n",
    "        return config\n",
    "    \n",
    "    def run_search(self):\n",
    "        \"\"\"Run progressive search across all stages.\"\"\"\n",
    "        best_arch = None\n",
    "        search_history = []\n",
    "        \n",
    "        for stage in self.search_stages:\n",
    "            print(f\"\\n=== {stage['name']} Stage ===\")\n",
    "            print(f\"Max depth: {stage['max_depth']}\")\n",
    "            print(f\"Operations: {stage['operations']}\")\n",
    "            print(f\"Trials: {stage['trials']}\")\n",
    "            \n",
    "            # Create search space for this stage\n",
    "            search_space = self.create_search_space(stage)\n",
    "            \n",
    "            # Generate candidate architectures\n",
    "            candidates = []\n",
    "            for _ in range(stage['trials']):\n",
    "                arch = search_space.sample_architecture()\n",
    "                candidates.append(arch)\n",
    "            \n",
    "            # Add best from previous stage as seed (if available)\n",
    "            if best_arch:\n",
    "                # Adapt previous best architecture to current stage constraints\n",
    "                adapted_arch = self.adapt_architecture(best_arch, stage)\n",
    "                if adapted_arch:\n",
    "                    candidates.append(adapted_arch)\n",
    "            \n",
    "            # Evaluate candidates\n",
    "            stage_results = []\n",
    "            for arch in candidates:\n",
    "                score = evaluate_architecture_placeholder(arch)\n",
    "                stage_results.append((arch, score))\n",
    "            \n",
    "            # Find best in this stage\n",
    "            stage_results.sort(key=lambda x: x[1], reverse=True)\n",
    "            stage_best_arch, stage_best_score = stage_results[0]\n",
    "            \n",
    "            print(f\"Best score: {stage_best_score:.4f}\")\n",
    "            print(f\"Best architecture depth: {len(stage_best_arch)}\")\n",
    "            \n",
    "            # Update global best\n",
    "            if best_arch is None or stage_best_score > search_history[-1]['best_score']:\n",
    "                best_arch = stage_best_arch\n",
    "            \n",
    "            # Record stage results\n",
    "            search_history.append({\n",
    "                'stage': stage['name'],\n",
    "                'best_score': stage_best_score,\n",
    "                'best_arch': stage_best_arch,\n",
    "                'num_candidates': len(candidates)\n",
    "            })\n",
    "        \n",
    "        return best_arch, search_history\n",
    "    \n",
    "    def adapt_architecture(self, architecture, stage_config):\n",
    "        \"\"\"\n",
    "        Adapt an architecture from previous stage to current stage constraints.\n",
    "        \"\"\"\n",
    "        adapted = []\n",
    "        \n",
    "        for layer in architecture:\n",
    "            # Check if layer type is allowed in this stage\n",
    "            if layer['type'] in stage_config['operations']:\n",
    "                # Adapt filter size if needed\n",
    "                adapted_layer = layer.copy()\n",
    "                if layer['filters'] > stage_config['max_filters']:\n",
    "                    adapted_layer['filters'] = stage_config['max_filters']\n",
    "                adapted.append(adapted_layer)\n",
    "        \n",
    "        # Truncate if too deep\n",
    "        if len(adapted) > stage_config['max_depth']:\n",
    "            adapted = adapted[:stage_config['max_depth']]\n",
    "        \n",
    "        return adapted if adapted else None\n",
    "\n",
    "# Example usage:\n",
    "progressive_nas = ProgressiveNAS()\n",
    "best_architecture, history = progressive_nas.run_search()\n",
    "\n",
    "print(f\"\\n=== Final Results ===\")\n",
    "print(f\"Best architecture found: {len(best_architecture)} layers\")\n",
    "\n",
    "# Show progression through stages\n",
    "for stage_result in history:\n",
    "    print(f\"{stage_result['stage']}: {stage_result['best_score']:.4f} \"\n",
    "          f\"({stage_result['num_candidates']} candidates)\")\n",
    "\n",
    "## 6. Practical Tools and Applications {#practical-tools}\n",
    "\n",
    "### 6.1 AutoKeras Integration\n",
    "\n",
    "# Note: This requires autokeras installation\n",
    "# pip install autokeras tensorflow\n",
    "\n",
    "def autokeras_image_classification_example():\n",
    "    \"\"\"\n",
    "    Complete example using AutoKeras for automated architecture search.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import autokeras as ak\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # Load sample data (CIFAR-10)\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        x_train = x_train.astype('float32') / 255.0\n",
    "        x_test = x_test.astype('float32') / 255.0\n",
    "        \n",
    "        print(f\"Training data shape: {x_train.shape}\")\n",
    "        print(f\"Test data shape: {x_test.shape}\")\n",
    "        \n",
    "        # Create AutoKeras image classifier\n",
    "        clf = ak.ImageClassifier(\n",
    "            max_trials=10,  # Number of different architectures to try\n",
    "            overwrite=True,\n",
    "            objective='val_accuracy',\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        # Search for best architecture and train\n",
    "        print(\"Starting architecture search...\")\n",
    "        clf.fit(\n",
    "            x_train, y_train,\n",
    "            validation_split=0.2,\n",
    "            epochs=5,  # Few epochs for demo\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_loss, test_acc = clf.evaluate(x_test, y_test, verbose=0)\n",
    "        print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "        \n",
    "        # Export the best model\n",
    "        best_model = clf.export_model()\n",
    "        print(f\"Best model has {best_model.count_params()} parameters\")\n",
    "        \n",
    "        # Save the model\n",
    "        # best_model.save('autokeras_best_model.h5')\n",
    "        \n",
    "        return clf, best_model\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"AutoKeras not installed. Install with: pip install autokeras\")\n",
    "        return None, None\n",
    "\n",
    "# Uncomment to run:\n",
    "# clf, model = autokeras_image_classification_example()\n",
    "\n",
    "### 6.2 Optuna Integration\n",
    "\n",
    "def optuna_nas_example():\n",
    "    \"\"\"\n",
    "    Example using Optuna for architecture search with custom objective.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import optuna\n",
    "        \n",
    "        def objective(trial):\n",
    "            \"\"\"\n",
    "            Objective function for Optuna optimization.\n",
    "            Defines the architecture search space and evaluation.\n",
    "            \"\"\"\n",
    "            # Define architecture hyperparameters\n",
    "            num_layers = trial.suggest_int('num_layers', 3, 15)\n",
    "            base_filters = trial.suggest_categorical('base_filters', [32, 64, 128])\n",
    "            kernel_size = trial.suggest_categorical('kernel_size', [3, 5, 7])\n",
    "            dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "            activation = trial.suggest_categorical('activation', ['relu', 'swish', 'gelu'])\n",
    "            \n",
    "            # Use these to create architecture\n",
    "            architecture = []\n",
    "            for i in range(num_layers):\n",
    "                # Gradually increase filters\n",
    "                filters = base_filters * (2 ** (i // 3))\n",
    "                layer_config = {\n",
    "                    'type': 'conv',\n",
    "                    'filters': min(filters, 512),  # Cap at 512\n",
    "                    'kernel_size': kernel_size,\n",
    "                    'activation': activation\n",
    "                }\n",
    "                architecture.append(layer_config)\n",
    "            \n",
    "            # Add dropout configuration\n",
    "            for layer in architecture:\n",
    "                layer['dropout'] = dropout_rate\n",
    "            \n",
    "            # Evaluate architecture (placeholder)\n",
    "            score = evaluate_architecture_placeholder(architecture)\n",
    "            \n",
    "            # Add small bonus for efficient architectures\n",
    "            efficiency_bonus = 0.01 / (1 + len(architecture) * base_filters / 1000)\n",
    "            final_score = score + efficiency_bonus\n",
    "            \n",
    "            return final_score\n",
    "        \n",
    "        # Create study and optimize\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name='nas_example',\n",
    "            pruner=optuna.pruners.MedianPruner()  # Early stopping\n",
    "        )\n",
    "        \n",
    "        print(\"Starting Optuna optimization...\")\n",
    "        study.optimize(objective, n_trials=50, timeout=300)  # 5 minute timeout\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nBest trial: {study.best_trial.number}\")\n",
    "        print(f\"Best score: {study.best_value:.4f}\")\n",
    "        print(\"Best parameters:\")\n",
    "        for key, value in study.best_params.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Create best architecture\n",
    "        best_params = study.best_params\n",
    "        best_architecture = []\n",
    "        for i in range(best_params['num_layers']):\n",
    "            filters = best_params['base_filters'] * (2 ** (i // 3))\n",
    "            layer_config = {\n",
    "                'type': 'conv',\n",
    "                'filters': min(filters, 512),\n",
    "                'kernel_size': best_params['kernel_size'],\n",
    "                'activation': best_params['activation'],\n",
    "                'dropout': best_params['dropout_rate']\n",
    "            }\n",
    "            best_architecture.append(layer_config)\n",
    "        \n",
    "        return study, best_architecture\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Optuna not installed. Install with: pip install optuna\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage:\n",
    "# study, best_arch = optuna_nas_example()\n",
    "# if best_arch:\n",
    "#     print(f\"Best architecture has {len(best_arch)} layers\")\n",
    "\n",
    "### 6.3 Ray Tune Integration\n",
    "\n",
    "def ray_tune_nas_example():\n",
    "    \"\"\"\n",
    "    Example using Ray Tune for distributed architecture search.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from ray import tune\n",
    "        from ray.tune.schedulers import ASHAScheduler\n",
    "        from ray.tune.suggest.optuna import OptunaSearch\n",
    "        \n",
    "        def train_architecture(config):\n",
    "            \"\"\"\n",
    "            Training function for Ray Tune.\n",
    "            This would contain actual model training in practice.\n",
    "            \"\"\"\n",
    "            # Build architecture from config\n",
    "            architecture = []\n",
    "            for i in range(config['num_layers']):\n",
    "                layer_config = {\n",
    "                    'type': config['layer_type'],\n",
    "                    'filters': config['base_filters'] * (2 ** (i // 3)),\n",
    "                    'kernel_size': config['kernel_size'],\n",
    "                    'activation': config['activation']\n",
    "                }\n",
    "                architecture.append(layer_config)\n",
    "            \n",
    "            # Simulate training epochs with intermediate reporting\n",
    "            for epoch in range(config['max_epochs']):\n",
    "                # Simulate training\n",
    "                intermediate_score = evaluate_architecture_placeholder(architecture)\n",
    "                \n",
    "                # Add epoch-based improvement\n",
    "                epoch_bonus = 0.1 * (1 - np.exp(-epoch / 20))\n",
    "                current_score = intermediate_score + epoch_bonus\n",
    "                \n",
    "                # Report intermediate result for early stopping\n",
    "                tune.report(accuracy=current_score, epoch=epoch)\n",
    "        \n",
    "        # Define search space\n",
    "        search_space = {\n",
    "            'num_layers': tune.randint(3, 15),\n",
    "            'base_filters': tune.choice([32, 64, 128]),\n",
    "            'kernel_size': tune.choice([3, 5, 7]),\n",
    "            'layer_type': tune.choice(['conv', 'depthwise_conv']),\n",
    "            'activation': tune.choice(['relu', 'swish', 'gelu']),\n",
    "            'max_epochs': 50\n",
    "        }\n",
    "        \n",
    "        # Configure scheduler for early stopping\n",
    "        scheduler = ASHAScheduler(\n",
    "            metric='accuracy',\n",
    "            mode='max',\n",
    "            max_t=50,  # Maximum epochs\n",
    "            grace_period=5,  # Don't eliminate before epoch 5\n",
    "            reduction_factor=2  # Keep half at each round\n",
    "        )\n",
    "        \n",
    "        # Configure search algorithm\n",
    "        search_alg = OptunaSearch()\n",
    "        \n",
    "        print(\"Starting Ray Tune optimization...\")\n",
    "        \n",
    "        # Run the search\n",
    "        analysis = tune.run(\n",
    "            train_architecture,\n",
    "            config=search_space,\n",
    "            num_samples=20,  # Number of trials\n",
    "            scheduler=scheduler,\n",
    "            search_alg=search_alg,\n",
    "            resources_per_trial={'cpu': 1},  # Adjust based on your setup\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Get best result\n",
    "        best_config = analysis.best_config\n",
    "        best_score = analysis.best_result['accuracy']\n",
    "        \n",
    "        print(f\"\\nBest configuration found:\")\n",
    "        print(f\"Score: {best_score:.4f}\")\n",
    "        for key, value in best_config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        return analysis, best_config\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Ray Tune not installed. Install with: pip install ray[tune]\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage:\n",
    "# analysis, best_config = ray_tune_nas_example()\n",
    "\n",
    "### 6.4 Complete NAS Pipeline\n",
    "\n",
    "class CompletePipeLineNAS:\n",
    "    \"\"\"\n",
    "    Complete NAS pipeline combining multiple techniques.\n",
    "    \"\"\"\n",
    "    def __init__(self, search_config=None):\n",
    "        self.search_config = search_config or SearchSpaceConfig()\n",
    "        self.zero_cost_filter = ZeroCostFilter()\n",
    "        self.results_history = []\n",
    "    \n",
    "    def run_complete_search(self, total_budget_hours=2):\n",
    "        \"\"\"\n",
    "        Run complete NAS pipeline with time budget.\n",
    "        \n",
    "        Args:\n",
    "            total_budget_hours: Total time budget in hours\n",
    "        \n",
    "        Returns:\n",
    "            Best architecture and search statistics\n",
    "        \"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        budget_seconds = total_budget_hours * 3600\n",
    "        \n",
    "        print(f\"Starting complete NAS pipeline with {total_budget_hours}h budget\")\n",
    "        \n",
    "        # Stage 1: Generate large pool of candidates\n",
    "        print(\"\\n=== Stage 1: Candidate Generation ===\")\n",
    "        num_initial_candidates = 1000\n",
    "        candidates = [\n",
    "            self.search_config.sample_architecture() \n",
    "            for _ in range(num_initial_candidates)\n",
    "        ]\n",
    "        print(f\"Generated {len(candidates)} initial candidates\")\n",
    "        \n",
    "        # Stage 2: Zero-cost filtering\n",
    "        print(\"\\n=== Stage 2: Zero-Cost Filtering ===\")\n",
    "        filtered_candidates = self.zero_cost_filter.filter_architectures(\n",
    "            candidates, keep_fraction=0.1\n",
    "        )\n",
    "        print(f\"Filtered to {len(filtered_candidates)} candidates\")\n",
    "        \n",
    "        # Stage 3: Multi-fidelity evaluation\n",
    "        print(\"\\n=== Stage 3: Multi-Fidelity Evaluation ===\")\n",
    "        if len(filtered_candidates) > 27:\n",
    "            # Further reduce if still too many\n",
    "            filtered_candidates = filtered_candidates[:27]\n",
    "        \n",
    "        best_arch = successive_halving_nas(\n",
    "            filtered_candidates, \n",
    "            max_epochs=20,  # Reduced for demo\n",
    "            reduction_factor=3\n",
    "        )\n",
    "        \n",
    "        # Stage 4: Final validation\n",
    "        print(\"\\n=== Stage 4: Final Validation ===\")\n",
    "        final_score = train_and_evaluate_placeholder(best_arch, epochs=50)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'best_architecture': best_arch,\n",
    "            'final_score': final_score,\n",
    "            'initial_candidates': num_initial_candidates,\n",
    "            'filtered_candidates': len(filtered_candidates),\n",
    "            'elapsed_time_hours': elapsed_time / 3600,\n",
    "            'budget_used_percent': (elapsed_time / budget_seconds) * 100\n",
    "        }\n",
    "        \n",
    "        self.results_history.append(results)\n",
    "        \n",
    "        print(f\"\\n=== Final Results ===\")\n",
    "        print(f\"Best architecture: {len(best_arch)} layers\")\n",
    "        print(f\"Final score: {final_score:.4f}\")\n",
    "        print(f\"Time used: {elapsed_time/3600:.2f}h ({results['budget_used_percent']:.1f}% of budget)\")\n",
    "        \n",
    "        return best_arch, results\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \"\"\"Analyze results from multiple runs.\"\"\"\n",
    "        if not self.results_history:\n",
    "            print(\"No results to analyze\")\n",
    "            return\n",
    "        \n",
    "        scores = [r['final_score'] for r in self.results_history]\n",
    "        times = [r['elapsed_time_hours'] for r in self.results_history]\n",
    "        \n",
    "        print(f\"\\n=== Analysis of {len(self.results_history)} runs ===\")\n",
    "        print(f\"Best score: {max(scores):.4f}\")\n",
    "        print(f\"Average score: {np.mean(scores):.4f}  {np.std(scores):.4f}\")\n",
    "        print(f\"Average time: {np.mean(times):.2f}h  {np.std(times):.2f}h\")\n",
    "\n",
    "# Example usage:\n",
    "pipeline = CompletePipeLineNAS()\n",
    "best_architecture, results = pipeline.run_complete_search(total_budget_hours=0.1)  # 6 minutes for demo\n",
    "\n",
    "# Run multiple times to see consistency\n",
    "# for i in range(3):\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Run {i+1}\")\n",
    "#     pipeline.run_complete_search(total_budget_hours=0.05)\n",
    "\n",
    "# pipeline.analyze_results()\n",
    "\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "#1. **Setup**: Install required dependencies using pip\n",
    "#2. **Basic Usage**: Start with the AutoKeras example for simplicity\n",
    "#3. **Custom Search**: Use the SearchSpaceConfig class to define your own search spaces\n",
    "#4. **Evaluation**: Implement your own `train_and_evaluate` function for real model training\n",
    "#5. **Scaling**: Use Ray Tune for distributed search across multiple machines\n",
    "\n",
    "## Notes\n",
    "\n",
    "#- Most functions include placeholder evaluations for demonstration\n",
    "#- Replace `evaluate_architecture_placeholder` with actual model training\n",
    "#- Adjust search spaces and budgets based on your computational resources\n",
    "#- Consider starting with smaller search spaces and gradually expanding\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "#- Original DARTS paper: https://arxiv.org/abs/1806.09055\n",
    "#- NAS survey: https://arxiv.org/abs/1808.05377\n",
    "#- AutoML book: Chapter 4 for detailed explanations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
