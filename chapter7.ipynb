{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7968ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from autogluon.text import TextPredictor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up logging and visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class NewsClassificationProject:\n",
    "    \"\"\"Complete news article classification pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, project_name: str = \"news_classifier\"):\n",
    "        self.data_path = data_path\n",
    "        self.project_name = project_name\n",
    "        self.predictor = None\n",
    "        self.categories = None\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_and_explore_data(self):\n",
    "        \"\"\"Load data and perform exploratory analysis\"\"\"\n",
    "        print(\"Loading and exploring dataset...\")\n",
    "        \n",
    "        # Load the data\n",
    "        self.data = pd.read_csv(self.data_path)\n",
    "        print(f\"Dataset shape: {self.data.shape}\")\n",
    "        print(f\"Columns: {self.data.columns.tolist()}\")\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_cols = ['title', 'content', 'category']\n",
    "        missing_cols = [col for col in required_cols if col not in self.data.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"\\nDataset Statistics:\")\n",
    "        print(f\"Total articles: {len(self.data):,}\")\n",
    "        print(f\"Unique categories: {self.data['category'].nunique()}\")\n",
    "        print(f\"Missing values: {self.data.isnull().sum().sum()}\")\n",
    "        \n",
    "        # Category distribution\n",
    "        self.categories = self.data['category'].value_counts()\n",
    "        print(f\"\\nCategory Distribution:\")\n",
    "        print(self.categories)\n",
    "        \n",
    "        # Text length analysis\n",
    "        self.data['title_length'] = self.data['title'].str.len()\n",
    "        self.data['content_length'] = self.data['content'].str.len()\n",
    "        self.data['total_length'] = self.data['title_length'] + self.data['content_length']\n",
    "        \n",
    "        print(f\"\\nText Length Statistics:\")\n",
    "        print(f\"Average title length: {self.data['title_length'].mean():.1f} characters\")\n",
    "        print(f\"Average content length: {self.data['content_length'].mean():.1f} characters\")\n",
    "        print(f\"Max total length: {self.data['total_length'].max():,} characters\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def visualize_data_distribution(self):\n",
    "        \"\"\"Create visualizations of data distribution\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Category distribution\n",
    "        axes[0,0].pie(self.categories.values, labels=self.categories.index, autopct='%1.1f%%')\n",
    "        axes[0,0].set_title('Category Distribution')\n",
    "        \n",
    "        # Text length distribution\n",
    "        axes[0,1].hist(self.data['total_length'], bins=50, alpha=0.7)\n",
    "        axes[0,1].set_title('Text Length Distribution')\n",
    "        axes[0,1].set_xlabel('Total Characters')\n",
    "        axes[0,1].set_ylabel('Frequency')\n",
    "        \n",
    "        # Text length by category\n",
    "        for i, category in enumerate(self.categories.head(5).index):\n",
    "            category_data = self.data[self.data['category'] == category]['total_length']\n",
    "            axes[1,0].hist(category_data, bins=30, alpha=0.6, label=category)\n",
    "        axes[1,0].set_title('Text Length by Category (Top 5)')\n",
    "        axes[1,0].set_xlabel('Total Characters')\n",
    "        axes[1,0].legend()\n",
    "        \n",
    "        # Category count bar plot\n",
    "        axes[1,1].bar(range(len(self.categories)), self.categories.values)\n",
    "        axes[1,1].set_title('Articles per Category')\n",
    "        axes[1,1].set_xlabel('Category')\n",
    "        axes[1,1].set_ylabel('Count')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.project_name}_data_analysis.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def prepare_training_data(self, min_samples_per_class=100, test_size=0.2):\n",
    "        \"\"\"Prepare balanced training and test sets\"\"\"\n",
    "        print(f\"\\nPreparing training data...\")\n",
    "        \n",
    "        # Filter categories with sufficient samples\n",
    "        valid_categories = self.categories[self.categories >= min_samples_per_class].index\n",
    "        filtered_data = self.data[self.data['category'].isin(valid_categories)].copy()\n",
    "        \n",
    "        print(f\"Filtered to {len(valid_categories)} categories with >= {min_samples_per_class} samples\")\n",
    "        print(f\"Remaining data: {len(filtered_data):,} articles\")\n",
    "        \n",
    "        # Combine title and content\n",
    "        filtered_data['text'] = filtered_data['title'] + ' ' + filtered_data['content']\n",
    "        \n",
    "        # Handle class imbalance by capping maximum samples per class\n",
    "        max_samples_per_class = min_samples_per_class * 5  # Allow up to 5x the minimum\n",
    "        balanced_data = []\n",
    "        \n",
    "        for category in valid_categories:\n",
    "            category_data = filtered_data[filtered_data['category'] == category]\n",
    "            if len(category_data) > max_samples_per_class:\n",
    "                category_data = category_data.sample(n=max_samples_per_class, random_state=42)\n",
    "            balanced_data.append(category_data)\n",
    "        \n",
    "        self.processed_data = pd.concat(balanced_data, ignore_index=True)\n",
    "        print(f\"Balanced dataset: {len(self.processed_data):,} articles\")\n",
    "        \n",
    "        # Split data\n",
    "        self.train_data, self.test_data = train_test_split(\n",
    "            self.processed_data[['text', 'category']],\n",
    "            test_size=test_size,\n",
    "            stratify=self.processed_data['category'],\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {len(self.train_data):,} articles\")\n",
    "        print(f\"Test set: {len(self.test_data):,} articles\")\n",
    "        \n",
    "        return self.train_data, self.test_data\n",
    "    \n",
    "    def train_models(self, time_limit=7200, presets='best_quality'):\n",
    "        \"\"\"Train the news classification model\"\"\"\n",
    "        print(f\"\\nTraining models with {presets} preset...\")\n",
    "        print(f\"Time limit: {time_limit/3600:.1f} hours\")\n",
    "        \n",
    "        # Custom hyperparameters optimized for news text\n",
    "        hyperparameters = {\n",
    "            'MultimodalTextModel': {\n",
    "                'optimization.learning_rate': [1e-5, 2e-5, 3e-5],\n",
    "                'optimization.max_epochs': [3, 5, 8],\n",
    "                'optimization.per_device_train_batch_size': [16, 32],\n",
    "                'model.hf_text.checkpoint_name': [\n",
    "                    'distilbert-base-uncased',\n",
    "                    'roberta-base',\n",
    "                    'microsoft/DialoGPT-medium'\n",
    "                ],\n",
    "                'model.hf_text.dropout_prob': [0.1, 0.2],\n",
    "                'optimization.gradient_clip_val': [1.0, 2.0]\n",
    "            },\n",
    "            'XGBModel': {\n",
    "                'n_estimators': [100, 200, 400],\n",
    "                'max_depth': [4, 6, 8],\n",
    "                'learning_rate': [0.05, 0.1, 0.2]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize predictor\n",
    "        self.predictor = TextPredictor(\n",
    "            label='category',\n",
    "            path=f'./{self.project_name}_model',\n",
    "            eval_metric='f1_macro',  # Good for multi-class with potential imbalance\n",
    "            verbosity=2\n",
    "        )\n",
    "        \n",
    "        # Train models\n",
    "        self.predictor.fit(\n",
    "            self.train_data,\n",
    "            time_limit=time_limit,\n",
    "            presets=presets,\n",
    "            hyperparameters=hyperparameters,\n",
    "            num_cpus=8,\n",
    "            num_gpus=1\n",
    "        )\n",
    "        \n",
    "        # Get training results\n",
    "        self.leaderboard = self.predictor.leaderboard(silent=False)\n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Best model: {self.leaderboard.index[0]}\")\n",
    "        print(f\"Best validation score: {self.leaderboard.iloc[0]['score_val']:.4f}\")\n",
    "        \n",
    "        return self.predictor\n",
    "    \n",
    "    def evaluate_model(self, save_results=True):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        print(f\"\\nEvaluating model performance...\")\n",
    "        \n",
    "        # Basic performance metrics\n",
    "        test_performance = self.predictor.evaluate(self.test_data)\n",
    "        predictions = self.predictor.predict(self.test_data)\n",
    "        probabilities = self.predictor.predict_proba(self.test_data)\n",
    "        \n",
    "        print(f\"Test Performance:\")\n",
    "        for metric, value in test_performance.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        # Detailed classification report\n",
    "        categories = sorted(self.test_data['category'].unique())\n",
    "        class_report = classification_report(\n",
    "            self.test_data['category'],\n",
    "            predictions,\n",
    "            target_names=categories,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Print per-class metrics\n",
    "        print(f\"\\nPer-Class Performance:\")\n",
    "        print(f\"{'Category':<20} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for category in categories:\n",
    "            if category in class_report:\n",
    "                metrics = class_report[category]\n",
    "                print(f\"{category:<20} {metrics['precision']:<10.3f} \"\n",
    "                      f\"{metrics['recall']:<10.3f} {metrics['f1-score']:<10.3f} \"\n",
    "                      f\"{int(metrics['support']):<10}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(self.test_data['category'], predictions, labels=categories)\n",
    "        \n",
    "        # Visualization\n",
    "        self.plot_evaluation_results(cm, categories, class_report)\n",
    "        \n",
    "        # Store results\n",
    "        self.results = {\n",
    "            'test_performance': test_performance,\n",
    "            'classification_report': class_report,\n",
    "            'confusion_matrix': cm,\n",
    "            'categories': categories,\n",
    "            'leaderboard': self.leaderboard\n",
    "        }\n",
    "        \n",
    "        if save_results:\n",
    "            self.save_evaluation_results()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def plot_evaluation_results(self, cm, categories, class_report):\n",
    "        \"\"\"Create comprehensive evaluation visualizations\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', \n",
    "                   xticklabels=categories, yticklabels=categories,\n",
    "                   cmap='Blues', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Normalized Confusion Matrix')\n",
    "        axes[0,0].set_xlabel('Predicted')\n",
    "        axes[0,0].set_ylabel('Actual')\n",
    "        \n",
    "        # Per-class F1 scores\n",
    "        f1_scores = [class_report[cat]['f1-score'] for cat in categories if cat in class_report]\n",
    "        axes[0,1].bar(range(len(categories)), f1_scores)\n",
    "        axes[0,1].set_title('F1-Score by Category')\n",
    "        axes[0,1].set_xlabel('Category')\n",
    "        axes[0,1].set_ylabel('F1-Score')\n",
    "        axes[0,1].set_xticks(range(len(categories)))\n",
    "        axes[0,1].set_xticklabels(categories, rotation=45)\n",
    "        \n",
    "        # Model performance comparison\n",
    "        model_scores = self.leaderboard['score_val'].head(10)\n",
    "        axes[1,0].barh(range(len(model_scores)), model_scores.values)\n",
    "        axes[1,0].set_title('Model Performance Comparison')\n",
    "        axes[1,0].set_xlabel('Validation Score')\n",
    "        axes[1,0].set_yticks(range(len(model_scores)))\n",
    "        axes[1,0].set_yticklabels(model_scores.index, fontsize=8)\n",
    "        \n",
    "        # Training time vs performance\n",
    "        if 'fit_time' in self.leaderboard.columns:\n",
    "            axes[1,1].scatter(self.leaderboard['fit_time'], self.leaderboard['score_val'])\n",
    "            axes[1,1].set_title('Training Time vs Performance')\n",
    "            axes[1,1].set_xlabel('Training Time (seconds)')\n",
    "            axes[1,1].set_ylabel('Validation Score')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.project_name}_evaluation.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def test_on_sample_articles(self):\n",
    "        \"\"\"Test the model on sample news articles\"\"\"\n",
    "        sample_articles = [\n",
    "            \"The Federal Reserve announced a 0.25% interest rate hike following concerns about inflation rates reaching multi-decade highs.\",\n",
    "            \"Local basketball team advances to championship finals after defeating rivals 98-87 in overtime thriller at packed arena.\",\n",
    "            \"Breakthrough artificial intelligence research promises to revolutionize medical diagnosis with 95% accuracy in detecting early-stage cancer.\",\n",
    "            \"Climate activists demand immediate action as global temperatures reach record highs for third consecutive year.\",\n",
    "            \"New smartphone features include advanced camera technology and longer battery life, launching next month.\",\n",
    "            \"President signs landmark infrastructure bill allocating $1.2 trillion for roads, bridges, and broadband expansion.\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nTesting on sample articles:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        predictions = self.predictor.predict(sample_articles)\n",
    "        probabilities = self.predictor.predict_proba(sample_articles)\n",
    "        \n",
    "        for i, article in enumerate(sample_articles):\n",
    "            print(f\"\\nArticle {i+1}: {article[:80]}...\")\n",
    "            print(f\"Predicted category: {predictions[i]}\")\n",
    "            \n",
    "            # Show top 3 predictions with probabilities\n",
    "            top_preds = probabilities.iloc[i].sort_values(ascending=False).head(3)\n",
    "            print(\"Top 3 predictions:\")\n",
    "            for j, (category, prob) in enumerate(top_preds.items()):\n",
    "                print(f\"  {j+1}. {category}: {prob:.3f}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    def save_evaluation_results(self):\n",
    "        \"\"\"Save evaluation results to files\"\"\"\n",
    "        results_dir = Path(f'{self.project_name}_results')\n",
    "        results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save detailed results\n",
    "        pd.DataFrame(self.results['classification_report']).T.to_csv(\n",
    "            results_dir / 'classification_report.csv'\n",
    "        )\n",
    "        \n",
    "        self.leaderboard.to_csv(results_dir / 'model_leaderboard.csv')\n",
    "        \n",
    "        # Save confusion matrix\n",
    "        cm_df = pd.DataFrame(\n",
    "            self.results['confusion_matrix'],\n",
    "            index=self.results['categories'],\n",
    "            columns=self.results['categories']\n",
    "        )\n",
    "        cm_df.to_csv(results_dir / 'confusion_matrix.csv')\n",
    "        \n",
    "        print(f\"Results saved to {results_dir}/\")\n",
    "    \n",
    "    def run_complete_pipeline(self, data_path=None, time_limit=7200):\n",
    "        \"\"\"Run the complete news classification pipeline\"\"\"\n",
    "        if data_path:\n",
    "            self.data_path = data_path\n",
    "            \n",
    "        print(\"Starting News Classification Project\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Step 1: Load and explore data\n",
    "        self.load_and_explore_data()\n",
    "        self.visualize_data_distribution()\n",
    "        \n",
    "        # Step 2: Prepare training data\n",
    "        self.prepare_training_data()\n",
    "        \n",
    "        # Step 3: Train models\n",
    "        self.train_models(time_limit=time_limit)\n",
    "        \n",
    "        # Step 4: Evaluate performance\n",
    "        self.evaluate_model()\n",
    "        \n",
    "        # Step 5: Test on samples\n",
    "        self.test_on_sample_articles()\n",
    "        \n",
    "        print(f\"\\nProject completed successfully!\")\n",
    "        print(f\"Model saved to: {self.project_name}_model/\")\n",
    "        print(f\"Results saved to: {self.project_name}_results/\")\n",
    "        \n",
    "        return self.predictor, self.results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the project\n",
    "    project = NewsClassificationProject(\n",
    "        data_path='data/news_dataset.csv',  # Replace with your dataset path\n",
    "        project_name='news_classifier_v1'\n",
    "    )\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    predictor, results = project.run_complete_pipeline(time_limit=3600)  # 1 hour\n",
    "    \n",
    "    # Additional analysis or deployment steps can be added here\n",
    "    print(f\"\\nFinal Results Summary:\")\n",
    "    print(f\"Best Model: {results['leaderboard'].index[0]}\")\n",
    "    print(f\"Test Accuracy: {results['test_performance'].get('accuracy', 0):.4f}\")\n",
    "    print(f\"Test F1-Macro: {results['test_performance'].get('f1_macro', 0):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
